{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9c442-72dd-4c73-86a3-98b627971d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "ANS>\n",
    "   An ensemble technique in machine learning involves combining the predictions of multiple models to produce a more robust and \n",
    "   accurate prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189d020-467c-43d3-b310-17e7b58b967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "ANS>Ensemble techniques are used in machine learning for several reasons:\n",
    "    \n",
    "    1> Improved Performance: Ensembles often outperform individual models by combining their strengths. By aggregating the predictions of\n",
    "                             multiple models, the ensemble can capture a broader range of patterns and relationships in the data.\n",
    "        \n",
    "    2> Reduced Overfitting: Ensembles can mitigate the risk of overfitting. Individual models might overfit to noise or small fluctuations\n",
    "                            in the training data, but an ensemble can smooth out these inconsistencies and provide more reliable predictions.\n",
    "        \n",
    "    3> Increased Robustness: Ensembles are less sensitive to outliers and errors in individual models. A single model making a mistake due to\n",
    "                             noisy data is less likely to influence the final prediction when combined with other models.  \n",
    "        \n",
    "    4> Generalization: Ensemble methods tend to have better generalization capabilities. They can extract higher-level insights from the data\n",
    "                       and perform well on new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf9c6b-f949-4274-8203-9c044d92a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is bagging?\n",
    "ANS> Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the accuracy and robustness \n",
    "     of predictive models. It involves training multiple instances of the same model on different subsets of the training data, typically \n",
    "     obtained through a process called bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f39f4-9818-485c-a847-46f0eaba88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is boosting?\n",
    "ANS> \n",
    "    Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by \n",
    "    building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which\n",
    "    tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training \n",
    "    data set is predicted correctly or the maximum number of models are added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90eb28c-02f5-4ff7-b922-95ec9aa5b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n",
    "ANS> \n",
    "   Ensemble techniques offer several benefits in machine learning:\n",
    "        \n",
    "    1> Improved Predictive Accuracy: Ensemble methods often yield higher predictive accuracy compared to individual models.\n",
    "                                     By combining multiple models, the ensemble can capture a broader range of patterns and relationships in\n",
    "                                     the data, leading to better predictions.\n",
    "            \n",
    "    2> Reduction of Overfitting: Ensembles can mitigate overfitting by combining the predictions of different models. If one model overfits to \n",
    "                                 noise or outliers, the ensemble can smooth out these inconsistencies and provide more reliable predictions.\n",
    "        \n",
    "    3> Enhanced Robustness: Ensembles are less sensitive to individual model errors. A single model making a mistake due to noisy data or model\n",
    "                            bias is less likely to influence the final prediction when combined with other models.\n",
    "        \n",
    "    4> Generalization: Ensembles tend to have better generalization capabilities. They can extract higher-level insights from the data and\n",
    "                       perform well on new, unseen data, reducing the risk of overfitting to the training data.\n",
    "        \n",
    "    5> Model Diversity: Effective ensembles combine diverse models with different biases and strengths. This diversity ensures that the ensemble\n",
    "                        can handle various aspects of the data and reduces the likelihood of all models making the same errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885c0cf-a756-4136-80a4-d251154d18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n",
    "ANS>\n",
    "   Ensemble techniques are not always guaranteed to be better than individual models. While ensemble methods can significantly improve predictive\n",
    "   performance in many cases, there are scenarios where they might not provide substantial benefits or could even lead to worse results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b93d1-ce67-4427-90f4-7c829d536be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "ANS> \n",
    "   The confidence interval using the bootstrap method involves resampling your original dataset to create multiple new datasets \n",
    "   (bootstrapped samples) and calculating a statistic of interest for each of these samples. The distribution of these bootstrapped \n",
    "   statistics is then used to estimate the confidence interval for the population parameter you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fde19-f3be-49ac-8c63-3aa6cba30cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "ANS> \n",
    "    Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from\n",
    "    the original data. It's particularly useful when you want to make inferences about a population parameter or estimate the variability of a\n",
    "    statistic when the analytical methods are not straightforward or when you have a limited amount of data.\n",
    "\n",
    "    Here are the steps involved in the bootstrap procedure:\n",
    "        \n",
    " 1> Original Data: Start with your original dataset containing n observations.\n",
    "\n",
    " 2> Resampling: Randomly sample n observations from the original dataset, with replacement. This means that each observation has an equal chance \n",
    "                of being selected in each resample, and some observations may be repeated while others might not be included at all.\n",
    "        \n",
    " 3> Calculate Statistic: Calculate the statistic of interest (e.g., mean, median, variance, etc.) for the current bootstrapped sample. \n",
    "                         This statistic is often denoted as the \"bootstrap statistic.\"\n",
    "    \n",
    " 4> Repeat Steps 2 and 3: Repeat steps 2 and 3 a large number of times (denoted as B, typically 1000 or more) to generate a collection of \n",
    "                          bootstrap statistics. Each bootstrap statistic is calculated from a different resampled dataset.\n",
    "\n",
    " 5> Statistic Distribution: The collection of bootstrap statistics forms an empirical distribution, known as the \"bootstrap distribution.\"\n",
    "                            You can visualize this distribution using histograms, kernel density plots, or other appropriate visualization techniques.\n",
    "\n",
    " 6> Confidence Interval: To estimate a confidence interval for the population parameter of interest (e.g., mean), you calculate the percentiles \n",
    "                         of the bootstrap distribution. For example, a 95% confidence interval might be obtained by calculating the 2.5th and \n",
    "                         97.5th percentiles of the bootstrap distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f8649-70f3-4fc2-b8d7-635fb8aeb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "    sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "    bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164a100c-570b-4980-997e-8d170bb1dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [14.43457957 15.54295387]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15  # Mean height of the sample\n",
    "sample_std = 2    # Standard deviation of the sample\n",
    "n = 50             # Sample size\n",
    "B = 1000           # Number of bootstrap samples\n",
    "\n",
    "# Generate bootstrap samples and calculate bootstrap means\n",
    "bootstrap_means = []\n",
    "for _ in range(B):\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, n)\n",
    "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c4dcc-a586-41bc-ac68-e775f7b06bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
